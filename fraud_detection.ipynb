{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __`Visualization of the Paysim dataset and preparation of the dataframe`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importazione librerie, autenticazione e creazione dell'entit√† \"grafo\"\n",
    "#libraries, authenticazion and creation of the graph entity\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as no\n",
    "from py2neo import Graph, Node, Relationship\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "\n",
    "url = \"insert-url-here\"\n",
    "pwd = \"insert-password-here\"\n",
    "gds = GraphDataScience(url, auth=('neo4j', pwd))\n",
    "graph = Graph(url, auth=('neo4j' , pwd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how initially appears the dataset in _neo4j_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/init.png\" width=\"600\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first implement the 'Weakly connected components' algorithm (based on \"union find\" data structure): It discovers hidden networks that constitute complex fraud rings based on common identities, such as multiple applicants all residing at the same address. We make joins by which we check whether certain individuals share an email or phone or ssn. We first need to create a new attribute that we call \"SHARED_IDENTIFIERS\". First we identify the client pairs that share personal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"MATCH (c1:Client)-[:HAS_EMAIL|HAS_PHONE|HAS_SSN]->(info)\n",
    "<-[:HAS_EMAIL|HAS_PHONE|HAS_SSN]-(c2:Client)\n",
    "WHERE c1.id<>c2.id\n",
    "WITH c1, c2, count(*) as cnt\n",
    "MERGE (c1) - [:SHARED_IDENTIFIERS {count: cnt}] - (c2);\"\"\"\n",
    "graph.run(query).to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows all possible relationships between the subjects, present by means of their ID, and the variable freq representing the number of identifiers they share."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/frequencies.png\" width=\"400\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute 'SHARED_IDENTIFIERS' was added, which relates people (real or fictitious) who share the same identity.\n",
    "Through this relationship, we create a temporary graph that we will need in order to effectively apply the 'Weakly Connected Component' algorithm. In particular, it is possible to detect clusters (groups of connected nodes in which each node represents a connected component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"CALL gds.graph.project('WCC_GRAPH', 'Client', \n",
    "{\n",
    "    SHARED_IDENTIFIERS: {\n",
    "        type: 'SHARED_IDENTIFIERS',\n",
    "        properties: {\n",
    "            count: {\n",
    "                property: 'count'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "})  YIELD graphName, nodeCount, relationshipCount;\"\"\"\n",
    "\n",
    "wcc_graph = graph.run(query).to_data_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of the \"Shared Identifiers\" relationship:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/shared_id.png\" width=\"600\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esecution of Weakly Connected Components algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>cluster</th><th>clusterSize</th><th>clientsInCluster</th></tr><tr><td style=\"text-align:right\">37</td><td style=\"text-align:right\">4</td><td style=\"text-align:left\">[&#039;4307389536474215&#039;, &#039;4454357737255718&#039;, &#039;4227754063695630&#039;, &#039;4297626800379477&#039;]</td></tr><tr><td style=\"text-align:right\">248</td><td style=\"text-align:right\">2</td><td style=\"text-align:left\">[&#039;4026791806639957&#039;, &#039;4169382142859298&#039;]</td></tr><tr><td style=\"text-align:right\">334</td><td style=\"text-align:right\">11</td><td style=\"text-align:left\">[&#039;4047841290742877&#039;, &#039;4777446725804270&#039;, &#039;4689748983194261&#039;, &#039;4284694673228754&#039;, &#039;4309833640194449&#039;, &#039;4316366410034140&#039;, &#039;4735236106636727&#039;, &#039;4187135907098538&#039;, &#039;4361847869567817&#039;, &#039;4910140986334626&#039;, &#039;4114683318919154&#039;]</td></tr></table>"
      ],
      "text/plain": [
       " cluster | clusterSize | clientsInCluster                                                                                                                                                                                                             \n",
       "---------|-------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
       "      37 |           4 | ['4307389536474215', '4454357737255718', '4227754063695630', '4297626800379477']                                                                                                                                             \n",
       "     248 |           2 | ['4026791806639957', '4169382142859298']                                                                                                                                                                                     \n",
       "     334 |          11 | ['4047841290742877', '4777446725804270', '4689748983194261', '4284694673228754', '4309833640194449', '4316366410034140', '4735236106636727', '4187135907098538', '4361847869567817', '4910140986334626', '4114683318919154'] "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"CALL gds.wcc.stream('WCC_GRAPH')\n",
    "YIELD componentId, nodeId\n",
    "WITH componentId AS cluster, gds.util.asNode(nodeId) AS client\n",
    "WITH cluster, collect(client.id) AS clients\n",
    "WITH *, size(clients) AS clusterSize\n",
    "WHERE clusterSize > 1\n",
    "UNWIND clients AS client\n",
    "MATCH (c:Client)\n",
    "WHERE c.id = client\n",
    "SET c.firstPartyFraudGroup = cluster\n",
    "WITH cluster, clusterSize, COLLECT(DISTINCT client) AS clientsInCluster\n",
    "RETURN cluster, clusterSize, clientsInCluster;\n",
    "\n",
    "\"\"\"\n",
    "graph.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the clusters we perform a subsequent algorithm, the Node Similarity: this is based on the idea that, two nodes are similar if they share the same neighbours. In particular, we use the Jaccard metric, also known as the Jaccard Similarity Score, obtained as the ratio between the related nodes in the network common to A and B (intersection), and the sum of these (union)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/jaccard_score.png\" width=\"600\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if there are two common identifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/example.png\" width=\"450\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are three possible values: 0.2, 0.5, 1.0 (based on the three possible common identifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the \"SIMILARITY\" relationship, let's apply the similarity algorithm based on \"Jaccard Score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"CALL gds.nodeSimilarity.stream('WCC_GRAPH', { topK: 15 })\n",
    "YIELD node1, node2, similarity\n",
    "WITH gds.util.asNode(node1) AS client1, gds.util.asNode(node2) AS client2, similarity\n",
    "OPTIONAL MATCH (client1)-[rel:SIMILARITY]-(client2)\n",
    "RETURN client1.id AS node1, client2.id AS node2, similarity, rel.similarity AS relationshipSimilarity\n",
    "ORDER BY similarity DESC\n",
    "LIMIT 15;\"\"\"\n",
    "similarity = graph.run(query).to_data_frame()\n",
    "similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also decide to count, for each client node, a Centrality Score given by the sum of all the weights of its arcs. Based on this and a certain threshold, the value of the Boolean variable prediction is set. \n",
    "A shared_identifiers column is also inserted with the maximum number of identifiers for each client node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataframe.csv')\n",
    "\n",
    "\n",
    "result_df = df.groupby('client1').agg({'score': 'sum', 'count': 'max'}).reset_index()\n",
    "\n",
    "result_df = result_df.rename(columns={'score': 'centrality', 'count': 'shared_identifiers'})\n",
    "\n",
    "\n",
    "\n",
    "result_df.to_csv('dataframe.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe that I will use for the machine learning phase appears like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/final_df.png\" width=\"450\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we have to balance the dataframe, as is appropriate in a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, a treshold value is also set, indicated by 2.3 (80th percentile), at which a client is considered fraudulent.  We thus have the two classes: positive and negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('dataframe.csv')\n",
    "\n",
    "\n",
    "result_df['prediction'] = result_df['centrality'].apply(lambda x: 1 if x >= 2.3 else 0)\n",
    "\n",
    "\n",
    "result_df.to_csv('dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/classes.png\" width=\"550\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore identify 1.7 as the value below which we eliminate the values in the presence of which the classes would be strongly unbalanced (271 positive against 65 negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "# __`Machine Learning`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the _Extreme Gradient Boosting_ algorithm. is an iterative model based on decision trees in which, at each iteration, an attempt is made to minimise an objective function. As the objective function decreases, the loss decreases and the efficiency of the algorithm increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/flowchart.png\" width=\"550\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow chart of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is defined as follows:<br>\n",
    "-Loss term:<br>\n",
    "        Measures how much the model's predictions deviate from actual values. For example, for regression, a common loss is the mean square loss (L2 loss). In our case (a binary classification model) is appropriate to use the logloss.<br>\n",
    "\n",
    "-Adjustment Term (Penalisation):<br>\n",
    "        Prevents the model from becoming too complex, preventing overfitting. It is based on parameters such as the number of leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/obj.png\" width=\"550\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also use cross validation to make the model generalise. In particular, I use 4 folds: these are 4 iterations in each of which, a different fold is used as the test set, while the other k-1 folds are used as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/crossval.png\" width=\"550\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics: Accuracy, Precision, Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"pics/metrics.png\" width=\"550\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code and results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "\n",
    "df = pd.read_csv('df.csv')\n",
    "df = df.drop(df[df['centrality'] < 1.7].index)\n",
    "\n",
    "#standardization of the features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['shared_identifiers', 'centrality']\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "#estraction of the important features\n",
    "X = df.drop(['client', 'prediction'], axis=1)\n",
    "y = df['prediction']\n",
    "\n",
    "#split the dataset in training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#XGBoost model\n",
    "model = xgb.XGBClassifier(\n",
    " n_estimators=100,\n",
    "    max_depth=3,\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False\n",
    "\n",
    ")\n",
    "\n",
    "#metrics\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score), \n",
    "    'f1' : make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "#cross-validation with 4 fold\n",
    "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "results = cross_validate(model, X_train, y_train, cv=cv, scoring=scoring)\n",
    "\n",
    "#average scores\n",
    "average_accuracy = results['test_accuracy'].mean()\n",
    "average_precision = results['test_precision'].mean()\n",
    "average_recall = results['test_recall'].mean()\n",
    "\n",
    "\n",
    "#results\n",
    "print(f'Test Accuracy: {average_accuracy:.4f}')\n",
    "print(f'Test Precision: {average_precision:.4f}')\n",
    "print(f'Test Recall: {average_recall:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\">\n",
    "    <img src=\"pics/scores.png\" width=\"450\" height=\"auto\"> <center>\n",
    "    <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
